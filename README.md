# ğŸš€ Azure-E-commerce-ETL-Pipeline  
*(ADF + ADLS + Databricks + Synapse + Power BI)*

This project implements a **production-style data pipeline on Azure** using the **Medallion Architecture (Bronze â†’ Silver â†’ Gold)**.  
It ingests raw data from multiple sources (**HTTP/GitHub, MySQL, MongoDB**), lands it in **Azure Data Lake Storage Gen2 (Delta Lake)**, transforms it with **Azure Databricks (PySpark)**, serves analytics through **Azure Synapse (Serverless/Dedicated)**, and visualizes insights in **Power BI**.

---

## ğŸ—ï¸ Project Architecture

![Azure ETL Pipeline Architecture](assets/architecture.png)

**Key design highlights:**
- **Medallion Architecture** (Bronze â†’ Silver â†’ Gold).
- **Delta Lake** (`_delta_log`, ACID, schema evolution).
- **ADF Pipelines**: parameterized, ForEach, Lookup, error handling.
- **Databricks PySpark**: cleansing, joins, surrogate keys, aggregates.
- **Synapse Analytics**: external tables, serverless SQL.
- **Power BI**: dashboards for sales trends, customer insights, and product performance.

## ğŸ¯ Goals & Highlights

- **Complete Lakehouse**: Bronze (raw) â†’ Silver (cleansed) â†’ Gold (business-ready).  
- **Multi-source ingestion**: HTTP/GitHub, SQL/MySQL, MongoDB.  
- **Delta Lake**: ACID transactions, schema evolution, time-travel.  
- **Partitioned facts**: e.g.,  
  - `fact_order_payments_partitioned` â†’ by `payment_type`  
  - `fact_sales_agg` â†’ by `year_month`  
  - `fact_sales_partitioned` â†’ by `purchase_date`  
- **Databricks ETL**: Surrogate keys, cleansing, SCD-friendly dims, aggregates.  
- **Synapse serving**: External tables & views.  
- **Power BI**: Actionable dashboards with KPIs & recommendations.  

---

## ğŸ“‚ Repository Structure

```

minzi03-azure-etl-pipeline/
â”‚â”€â”€ README.md
â”‚
â”œâ”€â”€ azure\_adls/                          # Lakehouse data (Delta format)
â”‚   â”œâ”€â”€ silver/                          # Cleansed entities (Delta tables)
â”‚   â””â”€â”€ gold/                            # Business-ready tables (facts/dims)
â”‚
â”œâ”€â”€ azure\_data\_factory/                  # ADF assets (JSON)
â”‚   â”œâ”€â”€ dataset/                         # Datasets (HTTP, CSV, SQL, JSON)
â”‚   â”œâ”€â”€ linkedService/                   # Linked services (ADLS, SQL, HTTP)
â”‚   â””â”€â”€ pipeline/data\_ingestion\_pipeline.json
â”‚
â”œâ”€â”€ azure\_databricks/                    # PySpark ETL scripts
â”‚   â”œâ”€â”€ Bronze-To-Silver.py
â”‚   â”œâ”€â”€ Silver-To-Gold(Dimensions).py
â”‚   â”œâ”€â”€ Silver-To-Gold(Facts).ipynb/.py/.dbc
â”‚   â””â”€â”€ data\_transformation.py
â”‚
â”œâ”€â”€ azure\_synapse/                       # Synapse SQL scripts
â”‚   â”œâ”€â”€ SQL\_script\_1.sql
â”‚   â””â”€â”€ SQL\_script\_2.sql
â”‚
â”œâ”€â”€ notebooks/                           # Ingestion examples
â”‚   â”œâ”€â”€ DataIngestion\_MongoDB.ipynb
â”‚   â”œâ”€â”€ DataIngestion\_MySQL.ipynb
â”‚   â”œâ”€â”€ DataIngestionToDB.ipynb
â”‚   â””â”€â”€ dataingestiontodb.py
â”‚
â”œâ”€â”€ data/                                # Raw helper dataset
â”‚   â””â”€â”€ product\_category\_name\_translation.csv
â”‚
â””â”€â”€ reports/
â””â”€â”€ powerbi/Analysis Highlights & Recommendations.docx

```

---

## ğŸ”„ ETL Pipeline Stages

### 1. **Data Ingestion â€” Bronze Layer**
- Extract raw data from GitHub/HTTP, MySQL, MongoDB.  
- Land into **ADLS Gen2 Bronze** (Delta/Parquet).  
- Orchestrated via **ADF pipelines** with `Copy`, `ForEach`, `Lookup`.  
- Incremental ingestion with **watermarking** strategy.  

### 2. **Transformation â€” Silver Layer**
- Cleaned & standardized with **Databricks PySpark**:  
  - Standardized column names & formats  
  - Null handling & deduplication  
  - Schema enforcement  
  - Derived fields (dates, geolocation normalization, etc.)  
- Stored as **Delta tables** in Silver for ACID compliance.  

### 3. **Modeling â€” Gold Layer**
- Built **Star Schema** with:  
  - **Fact Tables**: `fact_sales`, `fact_sales_agg`, `fact_order_payments_partitioned`  
  - **Dimension Tables**: `dim_customer`, `dim_product`, `dim_seller`, `dim_orders`, `dim_geolocation`, `dim_order_items`, `dim_order_payments`, `dim_order_reviews`  
  - **Bridge Tables**: `bridge_order_items`  
- Performance tuning with **Z-Ordering**, **Partitioning**, **Delta Vacuuming**.  

### 4. **Serving & Analytics**
- Gold tables surfaced in **Azure Synapse** via external tables.  
- Final insights consumed in **Power BI dashboards**.  

---

## ğŸ§© Data Model (Gold Layer)

**Dimensions**  
- `dim_customer`, `dim_product`, `dim_seller`, `dim_orders`,  
- `dim_geolocation`, `dim_order_items`, `dim_order_payments`, `dim_order_reviews`  

**Facts**  
- `fact_sales` â€” atomic line-item transactions  
- `fact_sales_agg` â€” aggregated by `year_month`  
- `fact_order_payments_partitioned` â€” partitioned by `payment_type`  
- `bridge_order_items` â€” handles many-to-many relationships  

---

## ğŸ› ï¸ Tech Stack

- **Azure Data Factory (ADF)** â†’ Orchestration, ingestion pipelines  
- **Azure Data Lake Storage Gen2 (ADLS)** â†’ Centralized Delta Lake  
- **Azure Databricks (PySpark, Delta Lake)** â†’ Data cleansing, transformations, schema modeling  
- **Azure Synapse Analytics** â†’ Query serving, SQL-based reporting  
- **Power BI** â†’ Visualization and storytelling  
- **Source Systems** â†’ MySQL, MongoDB, GitHub HTTP endpoints  

---

## ğŸš€ How to Run (Quick Start)

1. **Provision Azure resources**  
   Create Resource Group, ADLS, ADF, Databricks workspace, Synapse Analytics.

2. **Deploy ADF assets**  
   - Import JSONs from `azure_data_factory/` (linked services, datasets, pipeline).  
   - Update credentials/keys in Linked Services.  
   - Publish and test pipeline run.

3. **Ingest raw data to Bronze**  
   - Run ADF pipelines.  
   - Verify data landed in **ADLS/bronze**.

4. **Transform with Databricks**  
   - Run notebooks in `azure_databricks/`.  
   - Materialize Silver and Gold layers as Delta tables.

5. **Create Synapse external tables**  
   - Run scripts from `azure_synapse/`.  
   - Query Gold layer data via Synapse SQL.

6. **Visualize in Power BI**  
   - Connect to Synapse serverless endpoint or Delta tables via Spark connector.  
   - Build dashboards (see sample recommendations in `/reports/powerbi/`).  

---

## ğŸ“ˆ Business Value & Impact

- **Data Usability** â†’ Raw â†’ Cleaned â†’ Analytics-ready.  
- **Performance** â†’ Partitioning & Delta optimizations reduce query times.  
- **Scalability** â†’ Automated, cloud-native pipeline handles growing data volumes.  
- **Decision-Making** â†’ Power BI dashboards enable insights into sales trends, customer behavior, product performance.  

---

## ğŸ“š References

### ğŸ“Š Dataset
- [Olist E-Commerce Dataset (Brazilian Marketplace)](https://www.kaggle.com/datasets/olistbr/brazilian-ecommerce)  

### â˜ï¸ Azure Services & Documentation
- [Azure Data Factory](https://learn.microsoft.com/en-us/azure/data-factory/introduction) â€” Orchestration & data ingestion  
- [Azure Data Lake Storage Gen2](https://learn.microsoft.com/en-us/azure/storage/blobs/data-lake-storage-introduction) â€” Centralized data lake  
- [Azure Databricks](https://learn.microsoft.com/en-us/azure/databricks/) â€” Data transformation & PySpark processing  
- [Azure Synapse Analytics](https://learn.microsoft.com/en-us/azure/synapse-analytics/) â€” Data serving & analytical queries  
- [Azure Key Vault](https://learn.microsoft.com/en-us/azure/key-vault/general/basic-concepts) â€” Secret management  

### ğŸ› ï¸ Data Engineering Concepts & Tools
- [Medallion Architecture](https://learn.microsoft.com/en-us/azure/databricks/lakehouse/medallion) â€” Bronze, Silver, Gold design pattern  
- [Delta Lake](https://delta.io/) â€” ACID transactions, schema enforcement, time travel  
- [Apache Spark (PySpark)](https://spark.apache.org/docs/latest/api/python/) â€” Distributed processing  
- [SQL Serverless/Dedicated in Synapse](https://learn.microsoft.com/en-us/azure/synapse-analytics/sql/on-demand-workspace-overview)  

### ğŸ—„ï¸ Source Systems
- [MySQL](https://dev.mysql.com/doc/) â€” Relational database for transactional data  
- [MongoDB](https://www.mongodb.com/docs/) â€” NoSQL database for semi-structured ingestion  
- [GitHub HTTP endpoints](https://docs.github.com/en/rest) â€” Public data hosting (CSV/JSON ingestion)  

### ğŸ“ Additional Resources & Tutorials
- [Azure Storage / Databricks: Connect & Storage Tutorial](https://learn.microsoft.com/en-us/azure/databricks/connect/storage/tutorial-azure-storage) â€” A tutorial for connecting ADLS with Databricks and configuring storage. 
- [Synapse SQL: Develop Tables Using CETAS](https://learn.microsoft.com/en-us/azure/synapse-analytics/sql/develop-tables-cetas) â€” How to use CETAS to export data from ADLS/Delta into external tables in Synapse.
- [Filess.io](https://filess.io) â€” File sharing/hosting service (used for managing & sharing auxiliary data).

### ğŸ“ˆ Visualization
- [Power BI](https://learn.microsoft.com/en-us/power-bi/) â€” Dashboards & business reporting  

